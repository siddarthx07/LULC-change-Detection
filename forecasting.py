# -*- coding: utf-8 -*-
"""forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1idjrgStCSzD4_giqNcQMi5bQNozqAqFB
"""

! pip install gluonts

! pip install pytorch-lightning

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import math

def wmape(y_true, y_pred):
    return np.abs(y_true - y_pred).sum() / np.abs(y_true).sum()

file_path = '/content/drive/MyDrive/merged.csv'

data2 = pd.read_csv(file_path)

data2['cord'] = '(' + data2['x'].astype(str) + ',' + data2['y'].astype(str) + ')'
#print(data2)
#print(data2['cord'].value_counts())
#print(data2['cord'].nunique())

data2['Year'] = pd.to_datetime(data2['Year'], format='%Y')

print(data2)

type(data2.iloc[0]['cord'])

data2 = data2.sort_values(by=['cord'])
data2

len_df = len(data2)
bck_i = int(len_df/5)
cord_dic = {}
for i in range(0,bck_i-1):
  li = 5 * i
  ri = 5 * (i+1)
  print(li,ri)
  df_i = data2.iloc[li:ri]
  key = df_i.iloc[0]['cord']
  cord_dic[key] = list(df_i.index)

cord_dic

data2.iloc[0:5]

from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt

len(cord_dic_keys)

cord_dic_keys = list(cord_dic.keys())
predicted= {}

for ki in range(len(cord_dic_keys)):
  keys = cord_dic_keys[ki]
  value=cord_dic[keys]
  test=data2.loc[value]
  testt = testt.sort_values(by=['Year'])
  p, d, q = 1, 1, 1
  test_actual=testt.iloc[:len(testt)-1]
  valid=testt.iloc[len(testt)-1:]
  # print(test_actual)
  # print(' and ')
  # print(valid)

  #Arima
  model = ARIMA( test_actual['z'], order=(p, d, q))
  fitted_model = model.fit()

  #forecast
  n_steps = 1  # Number of steps to forecast
  forecast = fitted_model.forecast(steps=n_steps)
  #print(forecast)

  #store output
  out_dic = {'actual': valid.iloc[0]['z'], 'predicted': forecast.values[0]}
  print(ki) #out_dic)
  predicted[keys]=out_dic

  #delete vars
  del model
  del fitted_model

data2['z'].value_counts()

data2.dtypes

data2= data2[['z','Year','cord']]

train = data2.loc[data2['Year'] <= '2020-12-31']

valid = data2.loc[(data2['Year'] >= '2021-01-01') & (data2['Year'] <= '2021-12-31')]

train_new= dict(tuple(train.groupby('cord')))

test = df[df['cord']=='(206405,1926365)']

cord_list = list(train['cord'].unique()[0:100])

train_new = train[train['cord'].isin(cord_list)]
train_new

valid = valid[valid['cord'].isin(cord_list)]
valid

data2.shape[0]

from gluonts.dataset.pandas import PandasDataset

train['cord'].value_counts()

train1 = train[0:10000]
train2 = train[100:200]

train_ds = PandasDataset.from_long_dataframe(train_new, target='z', item_id='cord', timestamp='Year', freq='D')
print(train_ds)

!pip install pytorch-lightning

prediction_length = 1
freq =  "D"

from gluonts.mx import DeepAREstimator, Trainer
#from gluonts.torch.model.deepar import DeepAREstimator
from gluonts.evaluation import make_evaluation_predictions, Evaluator
def train_and_predict(dataset, estimator):
    predictor = estimator.train(dataset)
    forecast_it, ts_it = make_evaluation_predictions(
        dataset=dataset, predictor=predictor
    )
    evaluator = Evaluator(quantiles=(np.arange(20) / 20.0)[1:])
    agg_metrics, item_metrics = evaluator(ts_it, forecast_it, num_series=len(dataset))
    return agg_metrics["MSE"]

train_ds

estimator = DeepAREstimator(
    freq=freq, prediction_length=prediction_length, trainer=Trainer(epochs=10)
)
train_and_predict(train_ds, estimator)

test_ds = PandasDataset.from_long_dataframe(valid, target='z', item_id='cord', timestamp='Year', freq='D')
print(test_ds)

predictor = estimator.train(train_ds)
forecast_it, ts_it = make_evaluation_predictions(dataset=test_ds, predictor=predictor)
forecasts = list(forecast_it)
tests = list(ts_it)
evaluator = Evaluator(quantiles=(np.arange(20) / 20.0)[1:])
agg_metrics, item_metrics = evaluator(tests, forecasts, num_series=len(test_ds))

agg_metrics

tests

item_metrics

item_metrics['abs_error'].astype('int').value_counts()























train_ds1 = PandasDataset.from_long_dataframe(train1, target='z', item_id='cord',
                                       timestamp='Year', freq='D')
train_ds1

train_ds2 = PandasDataset.from_long_dataframe(train2, target='z', item_id='cord',
                                       timestamp='Year', freq='D')
train_ds2

vars(train_ds2)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


def generate_single_ts(date_range, item_id=None) -> pd.DataFrame:
    """create sum of `n_f` sin/cos curves with random scale and phase."""
    n_f = 2
    period = np.array([24 / (i + 1) for i in range(n_f)]).reshape(1, n_f)
    scale = np.random.normal(1, 0.3, size=(1, n_f))
    phase = 2 * np.pi * np.random.uniform(size=(1, n_f))
    periodic_f = lambda x: scale * np.sin(np.pi * x / period + phase)

    t = np.arange(0, len(date_range)).reshape(-1, 1)
    target = periodic_f(t).sum(axis=1) + np.random.normal(0, 0.1, size=len(t))
    ts = pd.DataFrame({"target": target}, index=date_range)
    if item_id is not None:
        ts["item_id"] = item_id
    return ts

# prediction_length, freq = 24, "1H"
# T = 10 * prediction_length
# date_range = pd.date_range("2021-01-01", periods=T, freq=freq)
# ts = generate_single_ts(date_range)

N = 10
multiple_ts = [generate_single_ts(date_range) for i in range(N)]

ds = PandasDataset(multiple_ts, target="target", freq=freq, )

ds = PandasDataset(train1, target="z",  timestamp='Year', freq='D')

ds

train_len = len(train)
train_len/10000

multiple_train = []
ran_len = math.ceil(train_len/10000)
for itl in range(ran_len):
  li = itl*10000
  ri = min(train_len, (itl+1)*10000)
  #print(li,ri)
  multiple_train.append(train[li:ri])
len(multiple_train)

train_ds = PandasDataset(multiple_train, target="z",  timestamp='Year', freq='D')
train_ds

ts_in_long_format = pd.concat(
    [generate_single_ts(date_range, item_id=i) for i in range(10)]
)
ts_in_long_format

ts_in_long_format = pd.concat(
    [generate_single_ts(date_range, item_id=i) for i in range(10)]
)
ts_in_long_format

train

train_ds

item_metrics

train1

tests

valid[0:20]

train['z'].value_counts()

n_plot = 3
indices = np.random.choice(np.arange(0, N), size=n_plot, replace=False)
fig, axes = plt.subplots(n_plot, 1, figsize=(10, n_plot * 5))
for index, ax in zip(indices, axes):
    ax.plot(tests[index][-4 * prediction_length :].to_timestamp())
    plt.sca(ax)
    forecasts[index].plot(intervals=(0.9,), color="g")
    plt.legend(["observed", "predicted median", "90% prediction interval"])

# ! pip install mxnet
# ! pip install lightning
! pip install "gluonts[mxnet]"

train_ds2._data_entries.fn

train_ds2._data_entries.__dict__

concatenated = pd.concat([train_ds1, train_ds2])